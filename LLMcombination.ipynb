{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adb98df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Any\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5cb1790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1814b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "class NewsRecommendationSystem:\n",
    "    def __init__(self, kg_analysis_path: str):\n",
    "        \"\"\"Initialize the recommendation system with KG analysis data\"\"\"\n",
    "        # Load API key\n",
    "        self.client = OpenAI(base_url=\"https://api.groq.com/openai/v1\", api_key= \"*******\")\n",
    "        \n",
    "        # Load KG analysis file\n",
    "        with open('../data/processed/kg_analysis1.json', 'r') as f:\n",
    "            self.kg_data = json.load(f)\n",
    "        \n",
    "        # Extract context\n",
    "        self.top_entities = {entity[0]: {'name': entity[2], 'frequency': entity[1]} \n",
    "                           for entity in self.kg_data['top_entities']}\n",
    "        self.stats = self.kg_data['stats']\n",
    "        \n",
    "        # Initialize data structure with KG context\n",
    "        self.user_interactions = self._load_user_interactions()\n",
    "        self.news_articles = self._load_news_articles()\n",
    "        self.entity_cooccurrence = self._load_entity_cooccurrence()\n",
    "        \n",
    "    def _parse_entities(self, entity_str):\n",
    "        \"\"\"Helper function to parse entity JSON strings\"\"\"\n",
    "        if pd.isna(entity_str) or entity_str == '[]':\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            entities = json.loads(entity_str)\n",
    "            return [entity.get('WikidataId', '') for entity in entities if entity.get('WikidataId')]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def _get_article_entities(self, article_id):\n",
    "        \"\"\"Helper function to get entities for a specific article\"\"\"\n",
    "        if hasattr(self, 'news_articles') and article_id in self.news_articles:\n",
    "            return self.news_articles[article_id][\"entities\"]\n",
    "        \n",
    "        # Fallback: load article on demand if not in memory\n",
    "        try:\n",
    "            news_path = '../data/mind/MINDsmall_train/news.tsv'\n",
    "            news_df = pd.read_csv(news_path, sep='\\t', \n",
    "                                names=['news_id', 'category', 'subcategory', 'title', 'abstract', \n",
    "                                       'url', 'title_entities', 'abstract_entities'])\n",
    "            \n",
    "            article_row = news_df[news_df['news_id'] == article_id]\n",
    "            if not article_row.empty:\n",
    "                row = article_row.iloc[0]\n",
    "                title_entities = self._parse_entities(row['title_entities'])\n",
    "                abstract_entities = self._parse_entities(row['abstract_entities'])\n",
    "                return list(set(title_entities + abstract_entities))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return []\n",
    "        \n",
    "    def _load_user_interactions(self) -> Dict:\n",
    "        \"\"\"Load actual user interaction data from MIND dataset\"\"\"\n",
    "        try:\n",
    "            behaviors_path = '../data/mind/MINDsmall_train/behaviors.tsv'\n",
    "            behaviors_df = pd.read_csv(behaviors_path, sep='\\t', \n",
    "                                     names=['impression_id', 'user_id', 'timestamp', 'history', 'impressions'])\n",
    "            \n",
    "            user_interactions = {}\n",
    "            \n",
    "            print(f\"Loading user interactions from {len(behaviors_df)} behavior records...\")\n",
    "            \n",
    "            # Process subset of users to avoid memory issues\n",
    "            for idx, row in behaviors_df.head(1000).iterrows():\n",
    "                user_id = row['user_id']\n",
    "                \n",
    "                if user_id not in user_interactions:\n",
    "                    user_interactions[user_id] = {\n",
    "                        \"clicked_entities\": [],\n",
    "                        \"viewed_entities\": [],\n",
    "                        \"interaction_counts\": {}\n",
    "                    }\n",
    "                \n",
    "                # Process history (articles user previously viewed)\n",
    "                if pd.notna(row['history']):\n",
    "                    history_articles = row['history'].split()\n",
    "                    for article_id in history_articles[:10]:  # Limit to recent 10 articles\n",
    "                        entities = self._get_article_entities(article_id)\n",
    "                        user_interactions[user_id][\"viewed_entities\"].extend(entities)\n",
    "                        for entity in entities:\n",
    "                            if entity in self.top_entities:  # Only count entities in our KG\n",
    "                                user_interactions[user_id][\"interaction_counts\"][entity] = \\\n",
    "                                    user_interactions[user_id][\"interaction_counts\"].get(entity, 0) + 1\n",
    "                \n",
    "                # Process impressions (articles shown with click labels)\n",
    "                if pd.notna(row['impressions']):\n",
    "                    impressions = row['impressions'].split()\n",
    "                    for impression in impressions:\n",
    "                        if '-' in impression:\n",
    "                            article_id, clicked = impression.split('-')\n",
    "                            entities = self._get_article_entities(article_id)\n",
    "                            \n",
    "                            # Add to viewed entities\n",
    "                            user_interactions[user_id][\"viewed_entities\"].extend(entities)\n",
    "                            \n",
    "                            # If clicked, add higher weight\n",
    "                            if clicked == '1':\n",
    "                                user_interactions[user_id][\"clicked_entities\"].extend(entities)\n",
    "                                for entity in entities:\n",
    "                                    if entity in self.top_entities:  # Only count entities in our KG\n",
    "                                        user_interactions[user_id][\"interaction_counts\"][entity] = \\\n",
    "                                            user_interactions[user_id][\"interaction_counts\"].get(entity, 0) + 3\n",
    "            \n",
    "            # Clean up duplicates\n",
    "            for user_id in user_interactions:\n",
    "                user_interactions[user_id][\"clicked_entities\"] = list(set(user_interactions[user_id][\"clicked_entities\"]))\n",
    "                user_interactions[user_id][\"viewed_entities\"] = list(set(user_interactions[user_id][\"viewed_entities\"]))\n",
    "            \n",
    "            print(f\"Loaded interactions for {len(user_interactions)} users\")\n",
    "            return user_interactions\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading user interactions: {e}\")\n",
    "            print(\"Falling back to mock data...\")\n",
    "            # Fallback to mock data if real data fails\n",
    "            return {\n",
    "                f\"user_{i}\": {\n",
    "                    \"clicked_entities\": random.sample(list(self.top_entities.keys()), \n",
    "                                                    min(random.randint(3, 8), len(self.top_entities.keys()))),\n",
    "                    \"viewed_entities\": random.sample(list(self.top_entities.keys()), \n",
    "                                                   min(random.randint(5, 10), len(self.top_entities.keys()))),\n",
    "                    \"interaction_counts\": {entity: random.randint(1, 10) \n",
    "                                         for entity in random.sample(list(self.top_entities.keys()), \n",
    "                                                                    min(random.randint(5, 10), len(self.top_entities.keys())))}\n",
    "                }\n",
    "                for i in range(100) \n",
    "            }\n",
    "    \n",
    "    def _load_news_articles(self) -> Dict:\n",
    "        \"\"\"Load actual news articles from MIND dataset\"\"\"\n",
    "        try:\n",
    "            news_path = '../data/mind/MINDsmall_train/news.tsv'\n",
    "            news_df = pd.read_csv(news_path, sep='\\t', \n",
    "                                names=['news_id', 'category', 'subcategory', 'title', 'abstract', \n",
    "                                       'url', 'title_entities', 'abstract_entities'])\n",
    "            \n",
    "            news_articles = {}\n",
    "            \n",
    "            print(f\"Loading {len(news_df)} news articles...\")\n",
    "            \n",
    "            for _, row in news_df.iterrows():\n",
    "                news_id = row['news_id']\n",
    "                \n",
    "                # Parse entities from JSON strings\n",
    "                title_entities = self._parse_entities(row['title_entities'])\n",
    "                abstract_entities = self._parse_entities(row['abstract_entities'])\n",
    "                all_entities = list(set(title_entities + abstract_entities))\n",
    "                \n",
    "                # Filter entities to only include those in our KG\n",
    "                kg_entities = [e for e in all_entities if e in self.top_entities]\n",
    "                \n",
    "                news_articles[news_id] = {\n",
    "                    \"title\": row['title'] if pd.notna(row['title']) else f\"News Article {news_id}\",\n",
    "                    \"abstract\": row['abstract'] if pd.notna(row['abstract']) else f\"Abstract for {news_id}\",\n",
    "                    \"entities\": kg_entities,\n",
    "                    \"category\": row['category'] if pd.notna(row['category']) else \"general\"\n",
    "                }\n",
    "            \n",
    "            print(f\"Loaded {len(news_articles)} articles with entities\")\n",
    "            return news_articles\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading news articles: {e}\")\n",
    "            print(\"Falling back to mock data...\")\n",
    "            # Fallback to mock data if real data fails\n",
    "            return {\n",
    "                f\"news_{i}\": {\n",
    "                    \"title\": f\"Sample News Article {i}\",\n",
    "                    \"abstract\": f\"This is a sample abstract for news article {i}\",\n",
    "                    \"entities\": random.sample(list(self.top_entities.keys()), \n",
    "                                            random.randint(2, 6)),\n",
    "                    \"category\": random.choice([\"politics\", \"sports\", \"technology\", \"world\"])\n",
    "                }\n",
    "                for i in range(1000) \n",
    "            }\n",
    "    \n",
    "    def _load_entity_cooccurrence(self) -> Dict:\n",
    "        \"\"\"Build entity co-occurrence matrix from real news articles\"\"\"\n",
    "        print(\"Building entity co-occurrence matrix...\")\n",
    "        cooccurrence = defaultdict(dict)\n",
    "        \n",
    "        # Count entity co-occurrences across all articles\n",
    "        for article_id, article_data in self.news_articles.items():\n",
    "            entities = article_data[\"entities\"]\n",
    "            \n",
    "            # For each pair of entities in the same article\n",
    "            for i, entity1 in enumerate(entities):\n",
    "                for entity2 in entities[i+1:]:\n",
    "                    if entity1 in self.top_entities and entity2 in self.top_entities:\n",
    "                        if entity2 not in cooccurrence[entity1]:\n",
    "                            cooccurrence[entity1][entity2] = 0\n",
    "                        if entity1 not in cooccurrence[entity2]:\n",
    "                            cooccurrence[entity2][entity1] = 0\n",
    "                        \n",
    "                        cooccurrence[entity1][entity2] += 1\n",
    "                        cooccurrence[entity2][entity1] += 1\n",
    "        \n",
    "        # Filter weak connections\n",
    "        filtered_cooccurrence = {}\n",
    "        min_cooccurrence = 2\n",
    "        \n",
    "        for entity1, connections in cooccurrence.items():\n",
    "            filtered_cooccurrence[entity1] = {}\n",
    "            for entity2, count in connections.items():\n",
    "                if count >= min_cooccurrence:\n",
    "                    filtered_cooccurrence[entity1][entity2] = count\n",
    "        \n",
    "        print(f\"Built co-occurrence matrix for {len(filtered_cooccurrence)} entities\")\n",
    "        return filtered_cooccurrence\n",
    "\n",
    "    def extract_kg_context(self, user_id: str, candidate_articles: List[str]) -> str:\n",
    "        \"\"\"Extract rich KG context for a user and candidate articles\"\"\"\n",
    "        user_data = self.user_interactions.get(user_id, {})\n",
    "        \n",
    "        # Get user's top entities based on interaction \n",
    "        user_entities = []\n",
    "        clicked_entities = user_data.get(\"clicked_entities\", [])\n",
    "        interaction_counts = user_data.get(\"interaction_counts\", {})\n",
    "        \n",
    "        # Prioritize clicked entities with interaction counts\n",
    "        for entity in clicked_entities[:5]: \n",
    "            entity_info = self.top_entities.get(entity, {})\n",
    "            count = interaction_counts.get(entity, 1)\n",
    "            user_entities.append(f\"{entity_info.get('name', entity)} (clicked {count} times)\")\n",
    "        \n",
    "        # Get co-occurring entities for interests shown by user\n",
    "        related_entities = []\n",
    "        for entity in clicked_entities[:3]:\n",
    "            if entity in self.entity_cooccurrence:\n",
    "                for related, strength in list(self.entity_cooccurrence[entity].items())[:3]:\n",
    "                    related_info = self.top_entities.get(related, {})\n",
    "                    related_entities.append(f\"{related_info.get('name', related)} (co-occurs {strength} times)\")\n",
    "        \n",
    "        # Analyze candidate articles\n",
    "        candidate_context = []\n",
    "        for article_id in candidate_articles:\n",
    "            article = self.news_articles.get(article_id, {})\n",
    "            article_entities = []\n",
    "            for entity in article.get(\"entities\", []):\n",
    "                entity_info = self.top_entities.get(entity, {})\n",
    "                article_entities.append(entity_info.get('name', entity))\n",
    "            \n",
    "            candidate_context.append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"category\": article.get(\"category\", \"\"),\n",
    "                \"entities\": article_entities\n",
    "            })\n",
    "        \n",
    "        # Construct KG context\n",
    "        context = f\"\"\"\n",
    "USER PROFILE FROM KNOWLEDGE GRAPH:\n",
    "Primary Interests (Clicked Entities): {', '.join(user_entities)}\n",
    "Related Interests (Co-occurring Entities): {', '.join(related_entities)}\n",
    "\n",
    "CANDIDATE ARTICLES WITH ENTITY ANALYSIS:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, article in enumerate(candidate_context, 1):\n",
    "            context += f\"\"\"\n",
    "Article {i}: {article['title']}\n",
    "Category: {article['category']}\n",
    "Key Entities: {', '.join(article['entities'])}\n",
    "\"\"\"\n",
    "        \n",
    "        context += f\"\"\"\n",
    "ENTITY IMPORTANCE CONTEXT:\n",
    "Global trending entities: {', '.join([self.top_entities[e]['name'] for e in list(self.top_entities.keys())[:5]])}\n",
    "\n",
    "RELATIONSHIP INSIGHTS:\n",
    "The knowledge graph shows {self.stats['nodes']} entities connected through {self.stats['edges']} relationships, \n",
    "indicating rich contextual associations between topics.\n",
    "\"\"\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def extract_llm_only_context(self, user_id: str, candidate_articles: List[str]) -> str:\n",
    "        \"\"\"Extract basic context without KG insights (for comparison)\"\"\"\n",
    "        user_data = self.user_interactions.get(user_id, {})\n",
    "        \n",
    "        # Get user history (basic)\n",
    "        clicked_entities = user_data.get(\"clicked_entities\", [])[:3]\n",
    "        basic_interests = [self.top_entities.get(e, {}).get('name', e) for e in clicked_entities]\n",
    "        \n",
    "        # Get article information (basic)\n",
    "        candidate_context = []\n",
    "        for article_id in candidate_articles:\n",
    "            article = self.news_articles.get(article_id, {})\n",
    "            candidate_context.append({\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"abstract\": article.get(\"abstract\", \"\"),\n",
    "                \"category\": article.get(\"category\", \"\")\n",
    "            })\n",
    "        \n",
    "        context = f\"\"\"\n",
    "User History:\n",
    "Previously interested in: {', '.join(basic_interests)}\n",
    "\n",
    "Candidate Articles:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, article in enumerate(candidate_context, 1):\n",
    "            context += f\"\"\"\n",
    "Article {i}: {article['title']}\n",
    "Summary: {article['abstract']}\n",
    "Category: {article['category']}\n",
    "\"\"\"\n",
    "        \n",
    "        return context\n",
    "\n",
    "    def generate_recommendations(self, context: str, method: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate recommendations using Groq LLM\"\"\"\n",
    "        \n",
    "        system_prompt = f\"\"\"You are a news recommendation system using {method} approach. \n",
    "        Analyze the provided context and recommend the most relevant articles for this user.\n",
    "        \n",
    "        Provide your response in this format:\n",
    "        RECOMMENDATIONS: [List top 3 recommended articles with brief reasoning]\n",
    "        EXPLANATIONS: [Explain why these articles match user interests]\n",
    "        DIVERSITY: [Comment on topic diversity in recommendations]\n",
    "        CONFIDENCE: [Rate confidence 1-10 and explain]\n",
    "        \"\"\"\n",
    "        \n",
    "        user_prompt = f\"\"\"\n",
    "        Context: {context}\n",
    "        \n",
    "        Please provide personalized news recommendations based on this context.\n",
    "        Focus on relevance, diversity, and explainability.\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"llama3-8b-8192\",  # or llama2-70b-4096\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                \"method\": method,\n",
    "                \"response\": response.choices[0].message.content,\n",
    "                \"success\": True\n",
    "            }\n",
    "        \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"method\": method,\n",
    "                \"response\": f\"Error: {str(e)}\",\n",
    "                \"success\": False\n",
    "            }\n",
    "\n",
    "    def compare_methods(self, user_id: str, num_candidates: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Compare KG+LLM vs LLM-only recommendations\"\"\"\n",
    "        \n",
    "        # Get candidate articles\n",
    "        candidate_articles = random.sample(list(self.news_articles.keys()), num_candidates)\n",
    "        \n",
    "        # Extract contexts using both methods\n",
    "        kg_context = self.extract_kg_context(user_id, candidate_articles)\n",
    "        llm_context = self.extract_llm_only_context(user_id, candidate_articles)\n",
    "        \n",
    "        # Generate recommendations using both methods\n",
    "        kg_llm_results = self.generate_recommendations(kg_context, \"KG+LLM\")\n",
    "        llm_only_results = self.generate_recommendations(llm_context, \"LLM-Only\")\n",
    "        \n",
    "        return {\n",
    "            \"user_id\": user_id,\n",
    "            \"candidate_articles\": candidate_articles,\n",
    "            \"kg_context_length\": len(kg_context),\n",
    "            \"llm_context_length\": len(llm_context),\n",
    "            \"kg_llm_recommendations\": kg_llm_results,\n",
    "            \"llm_only_recommendations\": llm_only_results,\n",
    "            \"context_comparison\": {\n",
    "                \"kg_context_preview\": kg_context[:500] + \"...\" if len(kg_context) > 500 else kg_context,\n",
    "                \"llm_context_preview\": llm_context[:500] + \"...\" if len(llm_context) > 500 else llm_context\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def run_evaluation(self, num_users: int = 5) -> List[Dict]:\n",
    "        \"\"\"Run evaluation comparing both methods across multiple users\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(\"Starting KG+LLM vs LLM-Only Evaluation...\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Get actual user IDs from loaded data\n",
    "        user_ids = list(self.user_interactions.keys())[:num_users]\n",
    "        \n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            print(f\"\\nEvaluating User {i+1}/{num_users}: {user_id}\")\n",
    "            \n",
    "            comparison_result = self.compare_methods(user_id)\n",
    "            results.append(comparison_result)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"KG Context Length: {comparison_result['kg_context_length']} chars\")\n",
    "            print(f\"LLM Context Length: {comparison_result['llm_context_length']} chars\")\n",
    "            print(f\"KG+LLM Success: {comparison_result['kg_llm_recommendations']['success']}\")\n",
    "            print(f\"LLM-Only Success: {comparison_result['llm_only_recommendations']['success']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results: List[Dict]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze and summarize comparison results\"\"\"\n",
    "        successful_kg = sum(1 for r in results if r['kg_llm_recommendations']['success'])\n",
    "        successful_llm = sum(1 for r in results if r['llm_only_recommendations']['success'])\n",
    "        \n",
    "        avg_kg_context_length = sum(r['kg_context_length'] for r in results) / len(results)\n",
    "        avg_llm_context_length = sum(r['llm_context_length'] for r in results) / len(results)\n",
    "        \n",
    "        analysis = {\n",
    "            \"total_evaluations\": len(results),\n",
    "            \"kg_llm_success_rate\": successful_kg / len(results),\n",
    "            \"llm_only_success_rate\": successful_llm / len(results),\n",
    "            \"avg_kg_context_length\": avg_kg_context_length,\n",
    "            \"avg_llm_context_length\": avg_llm_context_length,\n",
    "            \"context_enrichment_ratio\": avg_kg_context_length / avg_llm_context_length,\n",
    "            \"detailed_results\": results\n",
    "        }\n",
    "        \n",
    "        return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7bfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"**************\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4121374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user interactions from 156965 behavior records...\n",
      "Loaded interactions for 988 users\n",
      "Loading 51282 news articles...\n",
      "Loaded 51282 articles with entities\n",
      "Building entity co-occurrence matrix...\n",
      "Built co-occurrence matrix for 10 entities\n",
      "Starting KG+LLM vs LLM-Only Evaluation...\n",
      "============================================================\n",
      "\n",
      "Evaluating User 1/3: U13740\n",
      "KG Context Length: 1102 chars\n",
      "LLM Context Length: 1737 chars\n",
      "KG+LLM Success: True\n",
      "LLM-Only Success: True\n",
      "\n",
      "Evaluating User 2/3: U91836\n",
      "KG Context Length: 1079 chars\n",
      "LLM Context Length: 1915 chars\n",
      "KG+LLM Success: True\n",
      "LLM-Only Success: True\n",
      "\n",
      "Evaluating User 3/3: U73700\n",
      "KG Context Length: 1196 chars\n",
      "LLM Context Length: 2135 chars\n",
      "KG+LLM Success: True\n",
      "LLM-Only Success: True\n",
      "\n",
      "\n",
      " Summary\n",
      "\n",
      "\n",
      "Total Evaluations: 3\n",
      "KG+LLM Success Rate: 100.00%\n",
      "LLM-Only Success Rate: 100.00%\n",
      "Average KG Context Length: 1126 chars\n",
      "Average LLM Context Length: 1929 chars\n",
      "Context Enrichment Ratio: 0.58x\n",
      "\n",
      "\n",
      "Recommendation Comparison\n",
      "\n",
      "\n",
      "\n",
      "KG+LLM Recommendation:\n",
      "\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "[1] Article 3: Scientists Discover What Makes 'Water Bears' Virtually Indestructible\n",
      "[2] Article 4: 15 Found Alive in Truck in U.K. as Authorities Identify 39 Migrants Found Dead Weeks Earlier\n",
      "[3] Article 1: 13 Mesmerizing Photos of Jellyfish You Have to See\n",
      "\n",
      "EXPLANATIONS:\n",
      "These articles match the user's interests based on the primary interests and related interests extracted from the knowledge graph. Article 3 is relevant to the user's interest in Q5082150 (Jellyfish) and Q3714946 (Science). Article 4 is relevant to the user's interest in Q271880 (News) and the global trending entity \"United States\". Article 1 is a lifestyle article that matches the user's interest in Q5082150 (Jellyfish).\n",
      "\n",
      "DIVERSITY:\n",
      "The recommended articles cover a range of topics, including science, news, and lifestyle, ensuring a diverse set of recommendations.\n",
      "\n",
      "CONFIDENCE: 8/10\n",
      "The confidence score is high because the recommendations are based on the user's explicit interests and the relationships between entities in the knowledge graph. The articles are also from different categories, ensuring a diverse set of recommendations.\n",
      "\n",
      "Note: The confidence score could be higher if more user data was available or if the relationships between entities were more specific.\n",
      "\n",
      "LLM-ONLY Recommendation:\n",
      "\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "[1] Article 3: Scientists Discover What Makes 'Water Bears' Virtually Indestructible\n",
      "[2] Article 1: 13 Mesmerizing Photos of Jellyfish You Have to See\n",
      "[3] Article 4: 15 Found Alive in Truck in U.K. as Authorities Identify 39 Migrants Found Dead Weeks Earlier\n",
      "\n",
      "EXPLANATIONS:\n",
      "These articles are recommended based on the user's history of interest in topics related to science, specifically Q5082150 (jellyfish) and Q3714946 (water bears). Article 3, which discusses the indestructibility of water bears, is a direct match with their previous interests. Article 1, featuring mesmerizing jellyfish photos, is also relevant to their previous interests. Article 4, while covering a different topic, is recommended due to its newsworthiness and potential interest in human stories.\n",
      "\n",
      "DIVERSITY:\n",
      "The recommended articles cover three distinct topics: science, lifestyle, and news. This diversity ensures that the user will be exposed to a range of topics, making the recommendations more engaging and informative.\n",
      "\n",
      "CONFIDENCE:\n",
      "I rate my confidence in these recommendations as 8 out of 10. The user's history of interest in science-related topics, combined with the relevance of the recommended articles, makes a strong case for their likely interest in these articles. However, there is always a possibility that the user may have changed their interests, making it impossible to achieve a perfect confidence score.\n",
      "\n",
      "Note that Article 2, which covers Tiger Woods' performance in the Zozo Championship, is not recommended despite its sports-related content. This is because the user's previous interests did not specifically mention Tiger Woods or golf, making it less likely that they would be interested in this article.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \n",
    "    if not os.getenv(\"GROQ_API_KEY\"):\n",
    "        print(\"ERROR: Please set GROQ_API_KEY environment variable\")\n",
    "        print(\"Example: export GROQ_API_KEY='your_api_key_here'\")\n",
    "        return\n",
    "    \n",
    "    # Initialize system\n",
    "    system = NewsRecommendationSystem(\"kg_analysis.json\")\n",
    "    \n",
    "    # Evaluation\n",
    "    results = system.run_evaluation(num_users=3)\n",
    "    \n",
    "    # Results\n",
    "    analysis = system.analyze_results(results)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\" Summary\")\n",
    "    print(\"\\n\")\n",
    "    print(f\"Total Evaluations: {analysis['total_evaluations']}\")\n",
    "    print(f\"KG+LLM Success Rate: {analysis['kg_llm_success_rate']:.2%}\")\n",
    "    print(f\"LLM-Only Success Rate: {analysis['llm_only_success_rate']:.2%}\")\n",
    "    print(f\"Average KG Context Length: {analysis['avg_kg_context_length']:.0f} chars\")\n",
    "    print(f\"Average LLM Context Length: {analysis['avg_llm_context_length']:.0f} chars\")\n",
    "    print(f\"Context Enrichment Ratio: {analysis['context_enrichment_ratio']:.2f}x\")\n",
    "    \n",
    "    # Show recommendations\n",
    "    if results:\n",
    "        print(\"\\n\")\n",
    "        print(\"Recommendation Comparison\")\n",
    "        print(\"\\n\")\n",
    "        sample = results[0]\n",
    "        \n",
    "        print(\"\\nKG+LLM Recommendation:\")\n",
    "        print(\"\\n\")\n",
    "        if sample['kg_llm_recommendations']['success']:\n",
    "            print(sample['kg_llm_recommendations']['response'])\n",
    "        else:\n",
    "            print(\"Failed to generate recommendation\")\n",
    "        \n",
    "        print(\"\\nLLM-ONLY Recommendation:\")\n",
    "        print(\"\\n\")\n",
    "        if sample['llm_only_recommendations']['success']:\n",
    "            print(sample['llm_only_recommendations']['response'])\n",
    "        else:\n",
    "            print(\"Failed to generate recommendation\")\n",
    "\n",
    "if __name__ == \"__main__\":               \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4632973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bcfae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed0ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MIND validation data for evaluation...\n",
      "Validation behaviors: 73152 records\n",
      "Validation news articles: 42416 unique articles\n",
      "Unique users in validation: 50000\n",
      "\n",
      "Sample behavior record:\n",
      "   impression_id user_id               timestamp                                                                                                history                                                                                                                                                                                        impressions\n",
      "0              1  U80234  11/15/2019 12:37:50 PM  N55189 N46039 N51741 N53234 N11276 N264 N40716 N28088 N43955 N6616 N47686 N63573 N38895 N30924 N35671  N28682-0 N48740-0 N31958-1 N34130-0 N6916-0 N5472-0 N50775-0 N24802-0 N19990-0 N33176-0 N62365-0 N5940-0 N6400-0 N58098-0 N42844-0 N49285-0 N51470-0 N53572-0 N11930-0 N21679-0 N55237-0 N29862-0\n",
      "\n",
      "Sample news record:\n",
      "  news_id   category      subcategory                                                                   title                                                                   abstract                                            url                                                                                                                                                                                                                                                                                                                                                                                                                                                                          title_entities abstract_entities\n",
      "0  N55528  lifestyle  lifestyleroyals  The Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By  Shop the notebooks, jackets, and more that the royals can't live without.  https://assets.msn.com/labs/mind/AAGH0ET.html  [{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]                []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load MIND validation data\n",
    "print(\"Loading MIND validation data for evaluation...\")\n",
    "\n",
    "# user interactions\n",
    "behaviors_val = pd.read_csv('../data/mind/MINDsmal_test/behaviors.tsv', \n",
    "                           sep='\\t', \n",
    "                           names=['impression_id', 'user_id', 'timestamp', 'history', 'impressions'])\n",
    "\n",
    "# validation news articles\n",
    "news_val = pd.read_csv('../data/mind/MINDsmal_test/news.tsv', \n",
    "                      sep='\\t', \n",
    "                      names=['news_id', 'category', 'subcategory', 'title', 'abstract', \n",
    "                             'url', 'title_entities', 'abstract_entities'])\n",
    "\n",
    "print(f\"Validation behaviors: {len(behaviors_val)} records\")\n",
    "print(f\"Validation news articles: {len(news_val)} unique articles\")\n",
    "print(f\"Unique users in validation: {behaviors_val['user_id'].nunique()}\")\n",
    "\n",
    "# Display data structure\n",
    "print(\"\\nSample behavior record:\")\n",
    "print(behaviors_val.head(1).to_string())\n",
    "print(\"\\nSample news record:\")\n",
    "print(news_val.head(1).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ee5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation data for AUC calculation...\n",
      "Processing user impressions and clicks...\n",
      "Processed 1000 users with valid impressions\n",
      "Average impressions per user: 35.9\n",
      "Average clicks per user: 1.5\n",
      "\n",
      "Creating news-entity mapping...\n",
      "Mapped entities for 42416 news articles\n",
      "Sample news entities: [('N55528', ['Q43274', 'Q9682', 'Q80976']), ('N18955', ['Q622899'])]\n",
      "\n",
      "Sample evaluation record:\n",
      "User: U80234\n",
      "Impressions: 22\n",
      "Clicks: 1\n",
      "First 5 news_ids: ['N28682', 'N48740', 'N31958', 'N34130', 'N6916']\n",
      "First 5 labels: [0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Process validation data for evaluation\n",
    "print(\"Processing validation data for AUC calculation...\")\n",
    "\n",
    "def parse_impressions(impression_str):\n",
    "    \"\"\"Parse impression data as string to get news_ids and click labels\"\"\"\n",
    "    if pd.isna(impression_str):\n",
    "        return [], []\n",
    "    \n",
    "    items = impression_str.split()\n",
    "    news_ids = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in items:\n",
    "        if '-' in item:\n",
    "            news_id, label = item.split('-')\n",
    "            news_ids.append(news_id)\n",
    "            labels.append(int(label))\n",
    "    \n",
    "    return news_ids, labels\n",
    "\n",
    "def parse_entities(entity_str):\n",
    "    \"\"\"Parse entity JSON string to extract WikidataIds\"\"\"\n",
    "    if pd.isna(entity_str) or entity_str == '[]':\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        import json\n",
    "        entities = json.loads(entity_str)\n",
    "        return [entity.get('WikidataId', '') for entity in entities if entity.get('WikidataId')]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "# Processed evaluation dataset\n",
    "evaluation_data = []\n",
    "processed_users = 0\n",
    "\n",
    "print(\"Processing user impressions and clicks...\")\n",
    "\n",
    "for idx, row in behaviors_val.head(1000).iterrows(): \n",
    "    user_id = row['user_id']\n",
    "    impression_str = row['impressions']\n",
    "    \n",
    "    if pd.notna(impression_str):\n",
    "        news_ids, click_labels = parse_impressions(impression_str)\n",
    "        \n",
    "        if len(news_ids) > 0:  # Only users with valid impressions ids\n",
    "            evaluation_data.append({\n",
    "                'user_id': user_id,\n",
    "                'news_ids': news_ids,\n",
    "                'click_labels': click_labels,\n",
    "                'num_impressions': len(news_ids),\n",
    "                'num_clicks': sum(click_labels)\n",
    "            })\n",
    "            processed_users += 1\n",
    "\n",
    "print(f\"Processed {processed_users} users with valid impressions\")\n",
    "print(f\"Average impressions per user: {np.mean([d['num_impressions'] for d in evaluation_data]):.1f}\")\n",
    "print(f\"Average clicks per user: {np.mean([d['num_clicks'] for d in evaluation_data]):.1f}\")\n",
    "\n",
    "# news entity mapping for scoring\n",
    "print(\"\\nCreating news-entity mapping...\")\n",
    "news_entities = {}\n",
    "\n",
    "for idx, row in news_val.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    title_entities = parse_entities(row['title_entities'])\n",
    "    abstract_entities = parse_entities(row['abstract_entities'])\n",
    "    \n",
    "    # Merge title and abstract entities\n",
    "    all_entities = list(set(title_entities + abstract_entities))\n",
    "    news_entities[news_id] = all_entities\n",
    "\n",
    "print(f\"Mapped entities for {len(news_entities)} news articles\")\n",
    "print(f\"Sample news entities: {list(news_entities.items())[:2]}\")\n",
    "\n",
    "# evaluation \n",
    "if evaluation_data:\n",
    "    sample = evaluation_data[0]\n",
    "    print(f\"\\nSample evaluation record:\")\n",
    "    print(f\"User: {sample['user_id']}\")\n",
    "    print(f\"Impressions: {sample['num_impressions']}\")\n",
    "    print(f\"Clicks: {sample['num_clicks']}\")\n",
    "    print(f\"First 5 news_ids: {sample['news_ids'][:5]}\")\n",
    "    print(f\"First 5 labels: {sample['click_labels'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9cfb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading user interactions from 156965 behavior records...\n",
      "Loaded interactions for 988 users\n",
      "Loading 51282 news articles...\n",
      "Loaded 51282 articles with entities\n",
      "Building entity co-occurrence matrix...\n",
      "Built co-occurrence matrix for 10 entities\n",
      "Using existing recommendation system for scoring...\n"
     ]
    }
   ],
   "source": [
    "#Evaluation of recommendations generated by system\n",
    "if 'system' not in locals():\n",
    "    system = NewsRecommendationSystem(\"../data/processed/kg_analysis1.json\") \n",
    "\n",
    "print(\"Using existing recommendation system for scoring...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7916c499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating user entity profiles for scoring...\n",
      "Calculating prediction scores for evaluation users...\n",
      "Generated prediction scores for 100 users\n",
      "\n",
      "Sample scoring results:\n",
      "User: U80234\n",
      "Prediction scores: [0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "True labels: [0, 0, 1, 0, 0]\n",
      "Score range: 0.000 - 0.000\n",
      "Clicked article scores: [0.0]\n"
     ]
    }
   ],
   "source": [
    "# user entity from training data and calculate prediction scores\n",
    "print(\"Creating user entity profiles for scoring...\")\n",
    "\n",
    "def get_user_entity_profile(user_id, system):\n",
    "    \"\"\"Get user entity profile from existing KG system\"\"\"\n",
    "    # user interactions from the system\n",
    "    user_data = system.user_interactions.get(user_id, {})\n",
    "    \n",
    "    if not user_data:\n",
    "        # Fallback to random user for testing\n",
    "        user_data = system.user_interactions.get('user_0', {})\n",
    "    \n",
    "    clicked_entities = user_data.get('clicked_entities', [])\n",
    "    interaction_counts = user_data.get('interaction_counts', {})\n",
    "    \n",
    "    # weighted entity profile\n",
    "    entity_profile = {}\n",
    "    for entity in clicked_entities:\n",
    "        weight = interaction_counts.get(entity, 1)\n",
    "        entity_profile[entity] = weight\n",
    "    \n",
    "    return entity_profile\n",
    "\n",
    "def calculate_entity_overlap_score(user_profile, article_entities, system):\n",
    "    \"\"\"Calculate relevance score based on entity overlap\"\"\"\n",
    "    if not user_profile or not article_entities:\n",
    "        return 0.0\n",
    "    \n",
    "    # Calculate weighted overlap\n",
    "    overlap_score = 0.0\n",
    "    total_user_weight = sum(user_profile.values())\n",
    "    \n",
    "    for entity in article_entities:\n",
    "        if entity in user_profile:\n",
    "            # Entity relevance from KG analysis\n",
    "            entity_importance = system.top_entities.get(entity, {}).get('frequency', 1)\n",
    "            user_weight = user_profile[entity]\n",
    "            \n",
    "            # user preference and their relevance\n",
    "            entity_score = (user_weight / total_user_weight) * np.log(1 + entity_importance)\n",
    "            overlap_score += entity_score\n",
    "    \n",
    "    # Normalize by article entity count\n",
    "    if len(article_entities) > 0:\n",
    "        overlap_score = overlap_score / len(article_entities)\n",
    "    \n",
    "    return min(overlap_score, 1.0) \n",
    "\n",
    "# Prediction scores for AUC evaluation\n",
    "print(\"Calculating prediction scores for evaluation users...\")\n",
    "\n",
    "auc_evaluation_data = []\n",
    "\n",
    "for user_data in evaluation_data[:100]:  \n",
    "    user_id = user_data['user_id']\n",
    "    news_ids = user_data['news_ids']\n",
    "    click_labels = user_data['click_labels']\n",
    "    \n",
    "    # User entity profile\n",
    "    user_profile = get_user_entity_profile(user_id, system)\n",
    "    \n",
    "    # Prediction scores for each article\n",
    "    prediction_scores = []\n",
    "    valid_labels = []\n",
    "    \n",
    "    for news_id, label in zip(news_ids, click_labels):\n",
    "        article_entities = news_entities.get(news_id, [])\n",
    "        score = calculate_entity_overlap_score(user_profile, article_entities, system)\n",
    "        \n",
    "        prediction_scores.append(score)\n",
    "        valid_labels.append(label)\n",
    "    \n",
    "    if len(prediction_scores) > 0:\n",
    "        auc_evaluation_data.append({\n",
    "            'user_id': user_id,\n",
    "            'prediction_scores': prediction_scores,\n",
    "            'true_labels': valid_labels,\n",
    "            'num_impressions': len(prediction_scores)\n",
    "        })\n",
    "\n",
    "print(f\"Generated prediction scores for {len(auc_evaluation_data)} users\")\n",
    "\n",
    "# Results\n",
    "if auc_evaluation_data:\n",
    "    sample = auc_evaluation_data[0]\n",
    "    print(f\"\\nSample scoring results:\")\n",
    "    print(f\"User: {sample['user_id']}\")\n",
    "    print(f\"Prediction scores: {sample['prediction_scores'][:5]}\")\n",
    "    print(f\"True labels: {sample['true_labels'][:5]}\")\n",
    "    print(f\"Score range: {min(sample['prediction_scores']):.3f} - {max(sample['prediction_scores']):.3f}\")\n",
    "    \n",
    "    # articles clicked vs predicted scores\n",
    "    clicked_indices = [i for i, label in enumerate(sample['true_labels']) if label == 1]\n",
    "    if clicked_indices:\n",
    "        print(f\"Clicked article scores: {[sample['prediction_scores'][i] for i in clicked_indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90d07cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " AUC results..\n",
      "\n",
      " AUC \n",
      "Valid users for AUC calculation: 100/100\n",
      "Mean AUC: 0.5000\n",
      "Median AUC: 0.5000\n",
      "Standard Deviation: 0.0000\n",
      "Min AUC: 0.5000\n",
      "Max AUC: 0.5000\n",
      "\n",
      " AUC Distribution\n"
     ]
    }
   ],
   "source": [
    "# AUC for the KG+LLM system\n",
    "print(\" AUC results..\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calculate_auc_per_user(prediction_scores, true_labels):\n",
    "    \"\"\"Calculate AUC for a single user\"\"\"\n",
    "    #  at least one positive and one negative sample\n",
    "    if len(set(true_labels)) < 2:\n",
    "        return None  # Cannot calculate AUC with only one class\n",
    "    \n",
    "    try:\n",
    "        return roc_auc_score(true_labels, prediction_scores)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# AUC for each user\n",
    "user_aucs = []\n",
    "valid_users = 0\n",
    "\n",
    "for user_data in auc_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    auc_score = calculate_auc_per_user(prediction_scores, true_labels)\n",
    "    \n",
    "    if auc_score is not None:\n",
    "        user_aucs.append(auc_score)\n",
    "        valid_users += 1\n",
    "\n",
    "#  overall metrics\n",
    "if user_aucs:\n",
    "    mean_auc = np.mean(user_aucs)\n",
    "    median_auc = np.median(user_aucs)\n",
    "    std_auc = np.std(user_aucs)\n",
    "    \n",
    "    print(f\"\\n AUC \")\n",
    "    print(f\"Valid users for AUC calculation: {valid_users}/{len(auc_evaluation_data)}\")\n",
    "    print(f\"Mean AUC: {mean_auc:.4f}\")\n",
    "    print(f\"Median AUC: {median_auc:.4f}\")\n",
    "    print(f\"Standard Deviation: {std_auc:.4f}\")\n",
    "    print(f\"Min AUC: {min(user_aucs):.4f}\")\n",
    "    print(f\"Max AUC: {max(user_aucs):.4f}\")\n",
    "    \n",
    "    # Distribution analysis\n",
    "    auc_ranges = {\n",
    "        \"0.0-0.3 (Poor)\": sum(1 for auc in user_aucs if 0.0 <= auc < 0.3),\n",
    "        \"0.3-0.5 (Below Random)\": sum(1 for auc in user_aucs if 0.3 <= auc < 0.5),\n",
    "        \"0.5-0.7 (Fair)\": sum(1 for auc in user_aucs if 0.5 <= auc < 0.7),\n",
    "        \"0.7-0.9 (Good)\": sum(1 for auc in user_aucs if 0.7 <= auc < 0.9),\n",
    "        \"0.9-1.0 (Excellent)\": sum(1 for auc in user_aucs if 0.9 <= auc <= 1.0),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n AUC Distribution\")\n",
    "    for range_name, count in auc_ranges.items():\n",
    "        percentage = (count / len(user_aucs)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7457a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating MRR\n",
      "\n",
      " MRR Results\n",
      "Mean MRR: 0.2848\n",
      "Median MRR: 0.1667\n",
      "Standard Deviation: 0.3005\n",
      "Min MRR: 0.0103\n",
      "Max MRR: 1.0000\n",
      "\n",
      " MRR Distribution \n",
      "0.0 (No clicks ranked first): 0 users (0.0%)\n",
      "0.0-0.2 (Poor ranking): 58 users (58.0%)\n",
      "0.2-0.5 (Fair ranking): 30 users (30.0%)\n",
      "0.5-1.0 (Good ranking): 12 users (12.0%)\n",
      "\n",
      " Examples\n",
      "User 1: MRR = 0.333, Clicked items at ranks: [3]\n",
      "User 2: MRR = 0.500, Clicked items at ranks: [2]\n",
      "User 3: MRR = 0.071, Clicked items at ranks: [14]\n"
     ]
    }
   ],
   "source": [
    "# MRR for the KG+LLM system\n",
    "print(\"Calculating MRR\")\n",
    "\n",
    "def calculate_mrr_per_user(prediction_scores, true_labels):\n",
    "    \"\"\"Calculate MRR for a single user\"\"\"\n",
    "    # list of (score, label) and sort them\n",
    "    scored_items = list(zip(prediction_scores, true_labels))\n",
    "    scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # first clicked item in ranked list\n",
    "    for rank, (score, label) in enumerate(scored_items, 1):\n",
    "        if label == 1:  # First clicked item\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0  # No clicked items foubd\n",
    "\n",
    "# MRR for each user\n",
    "user_mrrs = []\n",
    "\n",
    "for user_data in auc_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    mrr_score = calculate_mrr_per_user(prediction_scores, true_labels)\n",
    "    user_mrrs.append(mrr_score)\n",
    "\n",
    "# MRR metrics\n",
    "mean_mrr = np.mean(user_mrrs)\n",
    "median_mrr = np.median(user_mrrs)\n",
    "std_mrr = np.std(user_mrrs)\n",
    "\n",
    "print(f\"\\n MRR Results\")\n",
    "print(f\"Mean MRR: {mean_mrr:.4f}\")\n",
    "print(f\"Median MRR: {median_mrr:.4f}\")\n",
    "print(f\"Standard Deviation: {std_mrr:.4f}\")\n",
    "print(f\"Min MRR: {min(user_mrrs):.4f}\")\n",
    "print(f\"Max MRR: {max(user_mrrs):.4f}\")\n",
    "\n",
    "# MRR Distribution\n",
    "mrr_ranges = {\n",
    "    \"0.0 (No clicks ranked first)\": sum(1 for mrr in user_mrrs if mrr == 0.0),\n",
    "    \"0.0-0.2 (Poor ranking)\": sum(1 for mrr in user_mrrs if 0.0 < mrr <= 0.2),\n",
    "    \"0.2-0.5 (Fair ranking)\": sum(1 for mrr in user_mrrs if 0.2 < mrr <= 0.5),\n",
    "    \"0.5-1.0 (Good ranking)\": sum(1 for mrr in user_mrrs if 0.5 < mrr <= 1.0),\n",
    "}\n",
    "\n",
    "print(f\"\\n MRR Distribution \")\n",
    "for range_name, count in mrr_ranges.items():\n",
    "    percentage = (count / len(user_mrrs)) * 100\n",
    "    print(f\"{range_name}: {count} users ({percentage:.1f}%)\")\n",
    "\n",
    "# sample rankings\n",
    "print(f\"\\n Examples\")\n",
    "for i, user_data in enumerate(auc_evaluation_data[:3]):\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    # ranked list\n",
    "    scored_items = list(zip(prediction_scores, true_labels, range(len(true_labels))))\n",
    "    scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # clicked items and their ranks\n",
    "    clicked_ranks = []\n",
    "    for rank, (score, label, orig_idx) in enumerate(scored_items, 1):\n",
    "        if label == 1:\n",
    "            clicked_ranks.append(rank)\n",
    "    \n",
    "    mrr = user_mrrs[i]\n",
    "    print(f\"User {i+1}: MRR = {mrr:.3f}, Clicked items at ranks: {clicked_ranks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52335a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating nDCG@5 and nDCG@10...\n",
      "\n",
      " nDCG@5 Results\n",
      "Mean nDCG@5: 0.2770\n",
      "Median nDCG@5: 0.0000\n",
      "Standard Deviation: 0.3387\n",
      "Min nDCG@5: 0.0000\n",
      "Max nDCG@5: 1.0000\n",
      "\n",
      " nDCG@10 Results \n",
      "Mean nDCG@10: 0.3395\n",
      "Median nDCG@10: 0.3244\n",
      "Standard Deviation: 0.3202\n",
      "Min nDCG@10: 0.0000\n",
      "Max nDCG@10: 1.0000\n",
      "\n",
      " nDCG@5 Distribution\n",
      "0.0-0.2: 55 users (55.0%)\n",
      "0.2-0.4: 8 users (8.0%)\n",
      "0.4-0.6: 15 users (15.0%)\n",
      "0.6-0.8: 12 users (12.0%)\n",
      "0.8-1.0: 10 users (10.0%)\n",
      "\n",
      " PERFORMANCE SUMMARY\n",
      "Metric       Mean     Median   Std     \n",
      "----------------------------------------\n",
      "AUC          0.5000   0.5000   0.0000  \n",
      "MRR          0.2848   0.1667   0.3005  \n",
      "nDCG@5       0.2770   0.0000   0.3387  \n",
      "nDCG@10      0.3395   0.3244   0.3202  \n"
     ]
    }
   ],
   "source": [
    "# nDCG@5 and nDCG@10 for the KG+LLM system\n",
    "print(\"Calculating nDCG@5 and nDCG@10...\")\n",
    "\n",
    "def calculate_dcg(relevances, k):\n",
    "    \"\"\"Calculate DCG@k\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        dcg += relevances[i] / np.log2(i + 2)  \n",
    "    return dcg\n",
    "\n",
    "def calculate_ndcg_per_user(prediction_scores, true_labels, k):\n",
    "    \"\"\"Calculate nDCG@k for a single user\"\"\"\n",
    "    # ranked list by prediction scores\n",
    "    scored_items = list(zip(prediction_scores, true_labels))\n",
    "    scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # labels in ranked order\n",
    "    ranked_relevances = [label for score, label in scored_items]\n",
    "    \n",
    "    # DCG@k\n",
    "    dcg_k = calculate_dcg(ranked_relevances, k)\n",
    "    \n",
    "    # IDCG@k \n",
    "    ideal_relevances = sorted(true_labels, reverse=True)\n",
    "    idcg_k = calculate_dcg(ideal_relevances, k)\n",
    "    \n",
    "    #  nDCG@k\n",
    "    if idcg_k == 0:\n",
    "        return 0.0\n",
    "    return dcg_k / idcg_k\n",
    "\n",
    "#  nDCG@5 and nDCG@10 for each user\n",
    "user_ndcg5 = []\n",
    "user_ndcg10 = []\n",
    "\n",
    "for user_data in auc_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    ndcg5 = calculate_ndcg_per_user(prediction_scores, true_labels, 5)\n",
    "    ndcg10 = calculate_ndcg_per_user(prediction_scores, true_labels, 10)\n",
    "    \n",
    "    user_ndcg5.append(ndcg5)\n",
    "    user_ndcg10.append(ndcg10)\n",
    "\n",
    "# Calculate overall nDCG metrics\n",
    "print(f\"\\n nDCG@5 Results\")\n",
    "print(f\"Mean nDCG@5: {np.mean(user_ndcg5):.4f}\")\n",
    "print(f\"Median nDCG@5: {np.median(user_ndcg5):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(user_ndcg5):.4f}\")\n",
    "print(f\"Min nDCG@5: {min(user_ndcg5):.4f}\")\n",
    "print(f\"Max nDCG@5: {max(user_ndcg5):.4f}\")\n",
    "\n",
    "print(f\"\\n nDCG@10 Results \")\n",
    "print(f\"Mean nDCG@10: {np.mean(user_ndcg10):.4f}\")\n",
    "print(f\"Median nDCG@10: {np.median(user_ndcg10):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(user_ndcg10):.4f}\")\n",
    "print(f\"Min nDCG@10: {min(user_ndcg10):.4f}\")\n",
    "print(f\"Max nDCG@10: {max(user_ndcg10):.4f}\")\n",
    "\n",
    "# Distribution analysis\n",
    "ndcg5_ranges = {\n",
    "    \"0.0-0.2\": sum(1 for ndcg in user_ndcg5 if 0.0 <= ndcg < 0.2),\n",
    "    \"0.2-0.4\": sum(1 for ndcg in user_ndcg5 if 0.2 <= ndcg < 0.4),\n",
    "    \"0.4-0.6\": sum(1 for ndcg in user_ndcg5 if 0.4 <= ndcg < 0.6),\n",
    "    \"0.6-0.8\": sum(1 for ndcg in user_ndcg5 if 0.6 <= ndcg < 0.8),\n",
    "    \"0.8-1.0\": sum(1 for ndcg in user_ndcg5 if 0.8 <= ndcg <= 1.0),\n",
    "}\n",
    "\n",
    "print(f\"\\n nDCG@5 Distribution\")\n",
    "for range_name, count in ndcg5_ranges.items():\n",
    "    percentage = (count / len(user_ndcg5)) * 100\n",
    "    print(f\"{range_name}: {count} users ({percentage:.1f}%)\")\n",
    "\n",
    "# Summary table\n",
    "print(f\"\\n PERFORMANCE SUMMARY\")\n",
    "print(f\"{'Metric':<12} {'Mean':<8} {'Median':<8} {'Std':<8}\")\n",
    "print(f\"{'-'*40}\")\n",
    "print(f\"{'AUC':<12} {np.mean([auc for auc in user_aucs]):<8.4f} {np.median([auc for auc in user_aucs]):<8.4f} {np.std([auc for auc in user_aucs]):<8.4f}\")\n",
    "print(f\"{'MRR':<12} {np.mean(user_mrrs):<8.4f} {np.median(user_mrrs):<8.4f} {np.std(user_mrrs):<8.4f}\")\n",
    "print(f\"{'nDCG@5':<12} {np.mean(user_ndcg5):<8.4f} {np.median(user_ndcg5):<8.4f} {np.std(user_ndcg5):<8.4f}\")\n",
    "print(f\"{'nDCG@10':<12} {np.mean(user_ndcg10):<8.4f} {np.median(user_ndcg10):<8.4f} {np.std(user_ndcg10):<8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd4e4b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating AUC, MRR, nDCG@5, nDCG@10 for LLM-only approach...\n"
     ]
    }
   ],
   "source": [
    "# LLM-only Baseline Evaluation \n",
    "\n",
    "\n",
    "print(\"Calculating AUC, MRR, nDCG@5, nDCG@10 for LLM-only approach...\")\n",
    "\n",
    "def create_llm_only_user_profiles(evaluation_data, behaviors_val, news_entities):\n",
    "    \"\"\" simple user profiles without KG context\"\"\"\n",
    "    print(\"Creating LLM-only user profiles...\")\n",
    "    \n",
    "    llm_user_profiles = {}\n",
    "    \n",
    "    for user_data in evaluation_data:\n",
    "        user_id = user_data['user_id']\n",
    "        \n",
    "        # users history from behaviors data \n",
    "        user_behaviors = behaviors_val[behaviors_val['user_id'] == user_id]\n",
    "        \n",
    "        if not user_behaviors.empty:\n",
    "            history = user_behaviors.iloc[0]['history']\n",
    "            \n",
    "            # entity counting from history \n",
    "            entity_counts = defaultdict(int)\n",
    "            \n",
    "            if pd.notna(history):\n",
    "                history_articles = history.split()[:10]  \n",
    "                \n",
    "                for article_id in history_articles:\n",
    "                    if article_id in news_entities:\n",
    "                        entities = news_entities[article_id]\n",
    "                        for entity in entities:\n",
    "                            entity_counts[entity] += 1\n",
    "            \n",
    "            llm_user_profiles[user_id] = dict(entity_counts)\n",
    "        else:\n",
    "            llm_user_profiles[user_id] = {}\n",
    "    \n",
    "    print(f\"Created profiles for {len(llm_user_profiles)} users\")\n",
    "    return llm_user_profiles\n",
    "\n",
    "def calculate_llm_only_score(user_profile, article_entities):\n",
    "    \"\"\"Calculate simple relevance score without KG context\"\"\"\n",
    "    if not user_profile or not article_entities:\n",
    "        return 0.0\n",
    "    \n",
    "    # overlap scoring  (count matching entities only)\n",
    "    overlap_score = 0.0\n",
    "    total_user_interactions = sum(user_profile.values())\n",
    "    \n",
    "    if total_user_interactions > 0:\n",
    "        for entity in article_entities:\n",
    "            if entity in user_profile:\n",
    "                # normalized score\n",
    "                overlap_score += user_profile[entity] / total_user_interactions\n",
    "        \n",
    "        # Normalize by article entity count\n",
    "        overlap_score = overlap_score / len(article_entities)\n",
    "    \n",
    "    return min(overlap_score, 1.0) \n",
    "\n",
    "def calculate_auc_per_user(prediction_scores, true_labels):\n",
    "    \"\"\"Calculate AUC for a single user\"\"\"\n",
    "    if len(set(true_labels)) < 2:\n",
    "        return None  \n",
    "    \n",
    "    try:\n",
    "        return roc_auc_score(true_labels, prediction_scores)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_mrr_per_user(prediction_scores, true_labels):\n",
    "    \"\"\"Calculate MRR for a single user\"\"\"\n",
    "    scored_items = list(zip(prediction_scores, true_labels))\n",
    "    scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    for rank, (score, label) in enumerate(scored_items, 1):\n",
    "        if label == 1:  # first clicked item by user\n",
    "            return 1.0 / rank\n",
    "    \n",
    "    return 0.0  # in case no clicked items found\n",
    "\n",
    "def calculate_dcg(relevances, k):\n",
    "    \"\"\"Calculate DCG@k\"\"\"\n",
    "    dcg = 0.0\n",
    "    for i in range(min(k, len(relevances))):\n",
    "        dcg += relevances[i] / np.log2(i + 2)\n",
    "    return dcg\n",
    "\n",
    "def calculate_ndcg_per_user(prediction_scores, true_labels, k):\n",
    "    \"\"\"Calculate nDCG@k for a single user\"\"\"\n",
    "    scored_items = list(zip(prediction_scores, true_labels))\n",
    "    scored_items.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    ranked_relevances = [label for score, label in scored_items]\n",
    "    dcg_k = calculate_dcg(ranked_relevances, k)\n",
    "    \n",
    "    ideal_relevances = sorted(true_labels, reverse=True)\n",
    "    idcg_k = calculate_dcg(ideal_relevances, k)\n",
    "    \n",
    "    if idcg_k == 0:\n",
    "        return 0.0\n",
    "    return dcg_k / idcg_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40dec0d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating LLM-only user profiles...\n",
      "Created profiles for 996 users\n",
      "Calculating LLM-only prediction scores...\n",
      "Generated LLM-only scores for 100 users\n",
      "\n",
      "\n",
      "LLM-only Baseline Results\n",
      "\n",
      "\n",
      "\n",
      "AUC Results:\n",
      "Valid users for AUC calculation: 100/100\n",
      "Mean AUC: 0.4999\n",
      "Median AUC: 0.5000\n",
      "Standard Deviation: 0.1088\n",
      "Min AUC: 0.3205\n",
      "Max AUC: 1.0000\n",
      "\n",
      "MRR Results:\n",
      "Mean MRR: 0.2870\n",
      "Median MRR: 0.1667\n",
      "Standard Deviation: 0.3100\n",
      "Min MRR: 0.0103\n",
      "Max MRR: 1.0000\n",
      "\n",
      "nDCG@5 Results:\n",
      "Mean nDCG@5: 0.2823\n",
      "Median nDCG@5: 0.0000\n",
      "Standard Deviation: 0.3441\n",
      "Min nDCG@5: 0.0000\n",
      "Max nDCG@5: 1.0000\n",
      "\n",
      "nDCG@10 Results:\n",
      "Mean nDCG@10: 0.3340\n",
      "Median nDCG@10: 0.3155\n",
      "Standard Deviation: 0.3271\n",
      "Min nDCG@10: 0.0000\n",
      "Max nDCG@10: 1.0000\n",
      "\n",
      "============================================================\n",
      "LLM-ONLY BASELINE SUMMARY\n",
      "============================================================\n",
      "Metric       Mean     Median   Std     \n",
      "----------------------------------------\n",
      "AUC          0.4999   0.5000   0.1088  \n",
      "MRR          0.2870   0.1667   0.3100  \n",
      "nDCG@5       0.2823   0.0000   0.3441  \n",
      "nDCG@10      0.3340   0.3155   0.3271  \n",
      "\n",
      "Performance Distribution:\n",
      "AUC Distribution:\n",
      "  0.0-0.3 (Poor): 0 users (0.0%)\n",
      "  0.3-0.5 (Below Random): 41 users (41.0%)\n",
      "  0.5-0.7 (Fair): 54 users (54.0%)\n",
      "  0.7-0.9 (Good): 2 users (2.0%)\n",
      "  0.9-1.0 (Excellent): 3 users (3.0%)\n",
      "\n",
      "Sample User Analysis (First 3 Users):\n",
      "User 1: 1 clicks out of 22 impressions\n",
      "  Average prediction score: 0.0026\n",
      "  Score range: 0.0000 - 0.0435\n",
      "  Clicked article scores: ['0.0000']\n",
      "User 2: 1 clicks out of 7 impressions\n",
      "  Average prediction score: 0.0000\n",
      "  Score range: 0.0000 - 0.0000\n",
      "  Clicked article scores: ['0.0000']\n",
      "User 3: 1 clicks out of 23 impressions\n",
      "  Average prediction score: 0.0005\n",
      "  Score range: 0.0000 - 0.0125\n",
      "  Clicked article scores: ['0.0000']\n",
      "\n",
      "\n",
      "LLM-only Baseline Evaluation done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create user profiles fro LLM-only approach\n",
    "llm_user_profiles = create_llm_only_user_profiles(evaluation_data, behaviors_val, news_entities)\n",
    "\n",
    "# prediction scores for LLM-only approach\n",
    "print(\"Calculating LLM-only prediction scores...\")\n",
    "\n",
    "llm_evaluation_data = []\n",
    "\n",
    "for user_data in evaluation_data[:100]:  #take first 100 users\n",
    "    user_id = user_data['user_id']\n",
    "    news_ids = user_data['news_ids']\n",
    "    click_labels = user_data['click_labels']\n",
    "    \n",
    "    # Get user profile\n",
    "    user_profile = llm_user_profiles.get(user_id, {})\n",
    "    \n",
    "    # Calculate prediction scores for each article\n",
    "    prediction_scores = []\n",
    "    \n",
    "    for news_id in news_ids:\n",
    "        article_entities = news_entities.get(news_id, [])\n",
    "        score = calculate_llm_only_score(user_profile, article_entities)\n",
    "        prediction_scores.append(score)\n",
    "    \n",
    "    if len(prediction_scores) > 0:\n",
    "        llm_evaluation_data.append({\n",
    "            'user_id': user_id,\n",
    "            'prediction_scores': prediction_scores,\n",
    "            'true_labels': click_labels,\n",
    "            'num_impressions': len(prediction_scores)\n",
    "        })\n",
    "\n",
    "print(f\"Generated LLM-only scores for {len(llm_evaluation_data)} users\")\n",
    "\n",
    "\n",
    "\n",
    "# METRICS \n",
    "# AUC\n",
    "\n",
    "llm_user_aucs = []\n",
    "valid_users_auc = 0\n",
    "\n",
    "for user_data in llm_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    auc_score = calculate_auc_per_user(prediction_scores, true_labels)\n",
    "    \n",
    "    if auc_score is not None:\n",
    "        llm_user_aucs.append(auc_score)\n",
    "        valid_users_auc += 1\n",
    "\n",
    "# MRR \n",
    "llm_user_mrrs = []\n",
    "\n",
    "for user_data in llm_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    mrr_score = calculate_mrr_per_user(prediction_scores, true_labels)\n",
    "    llm_user_mrrs.append(mrr_score)\n",
    "\n",
    "\n",
    "# nDCG@k\n",
    "llm_user_ndcg5 = []\n",
    "llm_user_ndcg10 = []\n",
    "\n",
    "for user_data in llm_evaluation_data:\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    ndcg5 = calculate_ndcg_per_user(prediction_scores, true_labels, 5)\n",
    "    ndcg10 = calculate_ndcg_per_user(prediction_scores, true_labels, 10)\n",
    "    \n",
    "    llm_user_ndcg5.append(ndcg5)\n",
    "    llm_user_ndcg10.append(ndcg10)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# results\n",
    "\n",
    "print(\"LLM-only Baseline Results\")\n",
    "print(\"\\n\")\n",
    "\n",
    "if llm_user_aucs:\n",
    "    print(f\"\\nAUC Results:\")\n",
    "    print(f\"Valid users for AUC calculation: {valid_users_auc}/{len(llm_evaluation_data)}\")\n",
    "    print(f\"Mean AUC: {np.mean(llm_user_aucs):.4f}\")\n",
    "    print(f\"Median AUC: {np.median(llm_user_aucs):.4f}\")\n",
    "    print(f\"Standard Deviation: {np.std(llm_user_aucs):.4f}\")\n",
    "    print(f\"Min AUC: {min(llm_user_aucs):.4f}\")\n",
    "    print(f\"Max AUC: {max(llm_user_aucs):.4f}\")\n",
    "\n",
    "print(f\"\\nMRR Results:\")\n",
    "print(f\"Mean MRR: {np.mean(llm_user_mrrs):.4f}\")\n",
    "print(f\"Median MRR: {np.median(llm_user_mrrs):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(llm_user_mrrs):.4f}\")\n",
    "print(f\"Min MRR: {min(llm_user_mrrs):.4f}\")\n",
    "print(f\"Max MRR: {max(llm_user_mrrs):.4f}\")\n",
    "\n",
    "print(f\"\\nnDCG@5 Results:\")\n",
    "print(f\"Mean nDCG@5: {np.mean(llm_user_ndcg5):.4f}\")\n",
    "print(f\"Median nDCG@5: {np.median(llm_user_ndcg5):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(llm_user_ndcg5):.4f}\")\n",
    "print(f\"Min nDCG@5: {min(llm_user_ndcg5):.4f}\")\n",
    "print(f\"Max nDCG@5: {max(llm_user_ndcg5):.4f}\")\n",
    "\n",
    "print(f\"\\nnDCG@10 Results:\")\n",
    "print(f\"Mean nDCG@10: {np.mean(llm_user_ndcg10):.4f}\")\n",
    "print(f\"Median nDCG@10: {np.median(llm_user_ndcg10):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(llm_user_ndcg10):.4f}\")\n",
    "print(f\"Min nDCG@10: {min(llm_user_ndcg10):.4f}\")\n",
    "print(f\"Max nDCG@10: {max(llm_user_ndcg10):.4f}\")\n",
    "\n",
    "# Summary Table\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"LLM-ONLY BASELINE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Metric':<12} {'Mean':<8} {'Median':<8} {'Std':<8}\")\n",
    "print(f\"{'-'*40}\")\n",
    "if llm_user_aucs:\n",
    "    print(f\"{'AUC':<12} {np.mean(llm_user_aucs):<8.4f} {np.median(llm_user_aucs):<8.4f} {np.std(llm_user_aucs):<8.4f}\")\n",
    "print(f\"{'MRR':<12} {np.mean(llm_user_mrrs):<8.4f} {np.median(llm_user_mrrs):<8.4f} {np.std(llm_user_mrrs):<8.4f}\")\n",
    "print(f\"{'nDCG@5':<12} {np.mean(llm_user_ndcg5):<8.4f} {np.median(llm_user_ndcg5):<8.4f} {np.std(llm_user_ndcg5):<8.4f}\")\n",
    "print(f\"{'nDCG@10':<12} {np.mean(llm_user_ndcg10):<8.4f} {np.median(llm_user_ndcg10):<8.4f} {np.std(llm_user_ndcg10):<8.4f}\")\n",
    "\n",
    "# Distribution Analysis\n",
    "print(f\"\\nPerformance Distribution:\")\n",
    "if llm_user_aucs:\n",
    "    auc_ranges = {\n",
    "        \"0.0-0.3 (Poor)\": sum(1 for auc in llm_user_aucs if 0.0 <= auc < 0.3),\n",
    "        \"0.3-0.5 (Below Random)\": sum(1 for auc in llm_user_aucs if 0.3 <= auc < 0.5),\n",
    "        \"0.5-0.7 (Fair)\": sum(1 for auc in llm_user_aucs if 0.5 <= auc < 0.7),\n",
    "        \"0.7-0.9 (Good)\": sum(1 for auc in llm_user_aucs if 0.7 <= auc < 0.9),\n",
    "        \"0.9-1.0 (Excellent)\": sum(1 for auc in llm_user_aucs if 0.9 <= auc <= 1.0),\n",
    "    }\n",
    "    \n",
    "    print(\"AUC Distribution:\")\n",
    "    for range_name, count in auc_ranges.items():\n",
    "        percentage = (count / len(llm_user_aucs)) * 100\n",
    "        print(f\"  {range_name}: {count} users ({percentage:.1f}%)\")\n",
    "\n",
    "# Sample Analysis\n",
    "print(f\"\\nSample User Analysis (First 3 Users):\")\n",
    "for i, user_data in enumerate(llm_evaluation_data[:3]):\n",
    "    prediction_scores = user_data['prediction_scores']\n",
    "    true_labels = user_data['true_labels']\n",
    "    \n",
    "    clicked_count = sum(true_labels)\n",
    "    avg_score = np.mean(prediction_scores)\n",
    "    \n",
    "    print(f\"User {i+1}: {clicked_count} clicks out of {len(true_labels)} impressions\")\n",
    "    print(f\"  Average prediction score: {avg_score:.4f}\")\n",
    "    print(f\"  Score range: {min(prediction_scores):.4f} - {max(prediction_scores):.4f}\")\n",
    "    \n",
    "    if clicked_count > 0:\n",
    "        clicked_scores = [prediction_scores[j] for j, label in enumerate(true_labels) if label == 1]\n",
    "        print(f\"  Clicked article scores: {[f'{score:.4f}' for score in clicked_scores]}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"LLM-only Baseline Evaluation done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba2558",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
